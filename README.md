# Well-formed Limericks and Haikus with GPT2
## ðŸ“œ GPT-2 Rhyming Limerick and Haiku models using data augmentation
### In collaboration with Matthew Korahais & Daniel Korsunsky
### Abstract 
We explore the capabilities and limits of large language models in the case of structured poetic forms, specifically limericks and haikus. We hypothesized that GPT-2 trained without phonetic annotations would be unable to sys- tematically learn and generate syllabic patterns and rhyme scheme, since these features are grounded in real world acoustic representations. Our model trained with list-of-rhymes annotations well-outperformed baselines, generating perfect-scoring limericks 33% of the time. Our best haiku model generated correct haikus in 29% of cases. Our work invites fur- ther research into methods of combining text and phonetic data for more convincing text generation.

Limericks Colab here -> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Rr4F4XSNZhC1jOVnUWHa0a3e9cQhEVvx?usp=sharing)]

Haiku Colab Here -> [![Open In Colab](https://colab.research.google.com/drive/1dY9eVMSHkeReJaDfl3YoQJG66DZxybcG#scrollTo=aeXshJM-Cuaf)]
